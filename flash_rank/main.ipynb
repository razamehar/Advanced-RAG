{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-15T14:41:56.959106Z",
     "start_time": "2025-06-15T14:41:56.925321Z"
    }
   },
   "source": [
    "import langchain\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "import re\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.load import dumps, loads\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import FlashrankRerank\n",
    "from flashrank.Ranker import Ranker"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T14:21:41.753055Z",
     "start_time": "2025-06-15T14:21:41.649239Z"
    }
   },
   "cell_type": "code",
   "source": "load_dotenv()",
   "id": "77a78102f524893d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T14:21:42.421576Z",
     "start_time": "2025-06-15T14:21:42.412581Z"
    }
   },
   "cell_type": "code",
   "source": "doc_path = '../data/pdf.pdf'",
   "id": "4cef55ff8bdd06e3",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T14:21:43.395112Z",
     "start_time": "2025-06-15T14:21:43.372669Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    fused_scores = {}\n",
    "    for docs in results:\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "    return reranked_results"
   ],
   "id": "82452e084aefba59",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T14:21:46.309838Z",
     "start_time": "2025-06-15T14:21:46.255144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = ChatPromptTemplate(input_variables=['original_query'],\n",
    "                            messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[],template='You are a helpful assistant that generates multiple search queries based on a single input query.')),\n",
    "                            HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['original_query'], template='Generate multiple search queries related to: {question} \\n OUTPUT (3 queries):'))])"
   ],
   "id": "bf17135b1c33d27a",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T14:21:46.995579Z",
     "start_time": "2025-06-15T14:21:46.983713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_text(text: str) -> str:\n",
    "\n",
    "    text = text.lower()\n",
    "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.replace('\\n', ' ').strip()\n",
    "    return text"
   ],
   "id": "56c5fd4a2a4b49ff",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T14:22:08.979276Z",
     "start_time": "2025-06-15T14:21:58.250643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loader=PyPDFLoader(doc_path)\n",
    "docs=loader.load()"
   ],
   "id": "820b0925a2e86de0",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T14:22:09.025948Z",
     "start_time": "2025-06-15T14:22:08.990849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for doc in docs:\n",
    "    doc.page_content = preprocess_text(doc.page_content)"
   ],
   "id": "a6889f728d97e313",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T14:22:09.119266Z",
     "start_time": "2025-06-15T14:22:09.044423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=200,chunk_overlap=30)\n",
    "chunks = splitter.split_documents(docs)"
   ],
   "id": "ad901f92bd49e38d",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T14:22:09.710200Z",
     "start_time": "2025-06-15T14:22:09.137384Z"
    }
   },
   "cell_type": "code",
   "source": "embeddings = OpenAIEmbeddings()",
   "id": "61f68d9965cb1186",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T14:22:18.878778Z",
     "start_time": "2025-06-15T14:22:10.218413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vectorstore=Chroma.from_documents(chunks,embeddings)\n",
    "vectorstore_retreiver = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "vectorstore_retreiver"
   ],
   "id": "a38aa0af22a880e5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000001A5A4060B50>, search_kwargs={'k': 5})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T14:22:19.034510Z",
     "start_time": "2025-06-15T14:22:18.967982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "keyword_retriever = BM25Retriever.from_documents(chunks)\n",
    "keyword_retriever.k = 3\n",
    "keyword_retriever"
   ],
   "id": "4e1aa3237de6eb66",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x000001A5A2E36790>, k=3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T14:22:25.216892Z",
     "start_time": "2025-06-15T14:22:25.194505Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ensemble_retriever = EnsembleRetriever(retrievers=[vectorstore_retreiver,keyword_retriever],weights=[0.3, 0.7])\n",
    "ensemble_retriever"
   ],
   "id": "8489b17f0661f6f9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnsembleRetriever(retrievers=[VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000001A5A4060B50>, search_kwargs={'k': 5}), BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x000001A5A2E36790>, k=3)], weights=[0.3, 0.7])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T14:22:27.942810Z",
     "start_time": "2025-06-15T14:22:27.897674Z"
    }
   },
   "cell_type": "code",
   "source": "llm = ChatOpenAI()",
   "id": "5614ed3b3f9432f5",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T14:22:29.075902Z",
     "start_time": "2025-06-15T14:22:29.048921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "generate_queries = (\n",
    "    prompt | llm | StrOutputParser() | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "generate_queries"
   ],
   "id": "9ff77f893bdaa9ce",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant that generates multiple search queries based on a single input query.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Generate multiple search queries related to: {question} \\n OUTPUT (3 queries):'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001A5A2E36D60>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001A5A2E36580>, root_client=<openai.OpenAI object at 0x000001A5A2E36940>, root_async_client=<openai.AsyncOpenAI object at 0x000001A5A2E36BE0>, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()\n",
       "| RunnableLambda(lambda x: x.split('\\n'))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T14:22:31.801360Z",
     "start_time": "2025-06-15T14:22:31.776218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ragfusion_chain = generate_queries | ensemble_retriever.map() | reciprocal_rank_fusion\n",
    "langchain.debug = True\n",
    "ragfusion_chain.input_schema.schema()"
   ],
   "id": "e73432d8349d2c80",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_10060\\249600610.py:3: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  ragfusion_chain.input_schema.schema()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'properties': {'question': {'title': 'Question', 'type': 'string'}},\n",
       " 'required': ['question'],\n",
       " 'title': 'PromptInput',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T14:22:36.246962Z",
     "start_time": "2025-06-15T14:22:36.222817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "full_rag_fusion_chain = (\n",
    "    {\n",
    "        \"context\": ragfusion_chain,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "full_rag_fusion_chain.input_schema.schema()"
   ],
   "id": "a1d91437412fbbaa",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_10060\\690582261.py:18: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  full_rag_fusion_chain.input_schema.schema()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'properties': {'question': {'title': 'Question', 'type': 'string'},\n",
       "  'root': {'title': 'Root'}},\n",
       " 'required': ['question', 'root'],\n",
       " 'title': 'RunnableParallel<context,question>Input',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "ranker = Ranker() # I have used Nano model. Small, medium and large options are also available.",
   "id": "b42988f8df109f0a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "small_ranker = Ranker(model_name=\"ms-marco-MiniLM-L-12-v2\", cache_dir=\"/opt\")\n",
    "medium_ranker = Ranker(model_name=\"rank-T5-flan\", cache_dir=\"/opt\")\n",
    "large_ranker = Ranker(model_name=\"ms-marco-MultiBERT-L-12\", cache_dir=\"/opt\")"
   ],
   "id": "c5387a5aab4a5abc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T14:49:28.038821Z",
     "start_time": "2025-06-15T14:48:43.500284Z"
    }
   },
   "cell_type": "code",
   "source": "compressor = FlashrankRerank()",
   "id": "3777bf11649e0b6c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:flashrank.Ranker:Downloading ms-marco-MultiBERT-L-12...\n",
      "ms-marco-MultiBERT-L-12.zip: 100%|██████████| 98.7M/98.7M [00:31<00:00, 3.33MiB/s]\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T14:49:33.417821Z",
     "start_time": "2025-06-15T14:49:33.408481Z"
    }
   },
   "cell_type": "code",
   "source": "compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=ensemble_retriever)",
   "id": "e339c17bb88df7cb",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T14:50:04.562998Z",
     "start_time": "2025-06-15T14:50:01.739049Z"
    }
   },
   "cell_type": "code",
   "source": "response1 = full_rag_fusion_chain.invoke(\"How to prevent data contamination?\")",
   "id": "2d83fe4055068819",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"How to prevent data contamination?\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableParallel<context,question>] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"How to prevent data contamination?\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"How to prevent data contamination?\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"How to prevent data contamination?\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence > prompt:ChatPromptTemplate] [2ms] Exiting Prompt run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[32;1m\u001B[1;3m[llm/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001B[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You are a helpful assistant that generates multiple search queries based on a single input query.\\nHuman: Generate multiple search queries related to: How to prevent data contamination? \\n OUTPUT (3 queries):\"\n",
      "  ]\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"How to prevent data contamination?\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnablePassthrough] [1ms] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": \"How to prevent data contamination?\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36;1m\u001B[1;3m[llm/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence > llm:ChatOpenAI] [660ms] Exiting LLM run with output:\n",
      "\u001B[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"1. Techniques for data cleanliness and integrity maintenance \\n2. Best practices for mitigating data contamination risks \\n3. Steps to ensure data purity and accuracy\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"1. Techniques for data cleanliness and integrity maintenance \\n2. Best practices for mitigating data contamination risks \\n3. Steps to ensure data purity and accuracy\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 30,\n",
      "                \"prompt_tokens\": 47,\n",
      "                \"total_tokens\": 77,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"accepted_prediction_tokens\": 0,\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"reasoning_tokens\": 0,\n",
      "                  \"rejected_prediction_tokens\": 0\n",
      "                },\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"cached_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"id\": \"chatcmpl-BiiyKQo29b2P9xCqMrdFIi3RrweVo\",\n",
      "              \"service_tier\": \"default\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run--81c115b2-3af8-430f-8f64-3b204da8848e-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 47,\n",
      "              \"output_tokens\": 30,\n",
      "              \"total_tokens\": 77,\n",
      "              \"input_token_details\": {\n",
      "                \"audio\": 0,\n",
      "                \"cache_read\": 0\n",
      "              },\n",
      "              \"output_token_details\": {\n",
      "                \"audio\": 0,\n",
      "                \"reasoning\": 0\n",
      "              }\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 30,\n",
      "      \"prompt_tokens\": 47,\n",
      "      \"total_tokens\": 77,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"accepted_prediction_tokens\": 0,\n",
      "        \"audio_tokens\": 0,\n",
      "        \"reasoning_tokens\": 0,\n",
      "        \"rejected_prediction_tokens\": 0\n",
      "      },\n",
      "      \"prompt_tokens_details\": {\n",
      "        \"audio_tokens\": 0,\n",
      "        \"cached_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null,\n",
      "    \"id\": \"chatcmpl-BiiyKQo29b2P9xCqMrdFIi3RrweVo\",\n",
      "    \"service_tier\": \"default\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": \"1. Techniques for data cleanliness and integrity maintenance \\n2. Best practices for mitigating data contamination risks \\n3. Steps to ensure data purity and accuracy\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": \"1. Techniques for data cleanliness and integrity maintenance \\n2. Best practices for mitigating data contamination risks \\n3. Steps to ensure data purity and accuracy\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": [\n",
      "    \"1. Techniques for data cleanliness and integrity maintenance \",\n",
      "    \"2. Best practices for mitigating data contamination risks \",\n",
      "    \"3. Steps to ensure data purity and accuracy\"\n",
      "  ]\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence > chain:RunnableEach<EnsembleRetriever>] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"input\": [\n",
      "    \"1. Techniques for data cleanliness and integrity maintenance \",\n",
      "    \"2. Best practices for mitigating data contamination risks \",\n",
      "    \"3. Steps to ensure data purity and accuracy\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence > chain:RunnableEach<EnsembleRetriever>] [538ms] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence > chain:reciprocal_rank_fusion] Entering Chain run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence > chain:reciprocal_rank_fusion] [8ms] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableParallel<context,question> > chain:RunnableSequence] [1.21s] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > chain:RunnableParallel<context,question>] [1.22s] Exiting Chain run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001B[0m[outputs]\n",
      "\u001B[32;1m\u001B[1;3m[llm/start]\u001B[0m \u001B[1m[chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001B[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Answer the question based only on the following context:\\n[(Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf.pdf', 'total_pages': 77, 'page': 68, 'page_label': '69'}, page_content='and sentiment reasoner (vader)(huttoandgilbert,2014)toevaluatethesentimentsconveyedbythecombinationofpromptprefix and model generation. vader produces a sentiment score between -1 and 1. a positive'), 0.016666666666666666), (Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf.pdf', 'total_pages': 77, 'page': 75, 'page_label': '76'}, page_content='to suggest contamination has affected evaluation performance on a dataset if all four sample subsets have|zn| > 2. results for this analysis can be seen in table 51. we observe that only hellaswag'), 0.016666666666666666), (Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf.pdf', 'total_pages': 77, 'page': 11, 'page_label': '12'}, page_content='data in a 90/10 proportion. we found that the setting with 10% helpfulness data is especially beneficial for the accuracy on samples where both the chosen and rejected responses were deemed safe.'), 0.016666666666666666), (Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf.pdf', 'total_pages': 77, 'page': 59, 'page_label': '60'}, page_content='options available to you, such as seeking financial assistance or finding alternative sources of income. it is important to always act with integrity and to prioritize the needs of your customers. rm'), 0.01639344262295082), (Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf.pdf', 'total_pages': 77, 'page': 22, 'page_label': '23'}, page_content='even before rlhf, and thus lays the foundation for high-quality human preference data annotation. 2. safety rlhf: subsequently, we integrate safety in the general rlhf pipeline described in sec- tion'), 0.01639344262295082), (Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf.pdf', 'total_pages': 77, 'page': 11, 'page_label': '12'}, page_content='helpfulness and safety reward models on the meta helpfulness and safety test sets. the reward models show superior accuracy on more distinct responses (e.g., significantly better) and lower accuracy'), 0.01639344262295082), (Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf.pdf', 'total_pages': 77, 'page': 68, 'page_label': '69'}, page_content='in the model generations. however, the truthfulness percentage is relatively low for pretrained models, around 30% to 40% for falcon, mpt, and the 7bllama 1. this percentage increases for'), 0.016129032258064516), (Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf.pdf', 'total_pages': 77, 'page': 39, 'page_label': '40'}, page_content='sebastian gehrmann, elizabeth clark, and thibault sellam. repairing the cracked foundation: a survey of obstacles in evaluation practices for generated text.journal of artificial intelligence'), 0.016129032258064516), (Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf.pdf', 'total_pages': 77, 'page': 16, 'page_label': '17'}, page_content='to be a good measure. to ensure ourmeasurewontdivergefromthehumanpreferences,weadditionallyusedamoregeneralreward,trained 17'), 0.016129032258064516), (Document(metadata={'producer': 'pdfTeX-1.40.25', 'page_label': '2', 'trapped': '/False', 'total_pages': 77, 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'author': '', 'creationdate': '2023-07-20T00:30:36+00:00', 'moddate': '2023-07-20T00:30:36+00:00', 'title': '', 'page': 1, 'subject': '', 'keywords': '', 'source': '../data/pdf.pdf', 'creator': 'LaTeX with hyperref'}, page_content='. . . . . . . 72 a.6 dataset contamination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 a.7 model card . . . . . . . . . . . . . . . . . . . . . . . . . . . .'), 0.015873015873015872), (Document(metadata={'page_label': '2', 'page': 1, 'author': '', 'producer': 'pdfTeX-1.40.25', 'source': '../data/pdf.pdf', 'title': '', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'keywords': '', 'creationdate': '2023-07-20T00:30:36+00:00', 'subject': '', 'moddate': '2023-07-20T00:30:36+00:00', 'total_pages': 77, 'creator': 'LaTeX with hyperref'}, page_content='. . . . . . . 72 a.6 dataset contamination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 a.7 model card . . . . . . . . . . . . . . . . . . . . . . . . . . . .'), 0.015873015873015872), (Document(metadata={'total_pages': 77, 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'page_label': '9', 'moddate': '2023-07-20T00:30:36+00:00', 'source': '../data/pdf.pdf', 'creationdate': '2023-07-20T00:30:36+00:00', 'title': '', 'creator': 'LaTeX with hyperref', 'trapped': '/False', 'subject': '', 'keywords': '', 'producer': 'pdfTeX-1.40.25', 'page': 8, 'author': ''}, page_content='different down- stream model performance, highlighting the importance of data checks even when using vendors to source annotations.'), 0.015873015873015872), (Document(metadata={'page_label': '9', 'moddate': '2023-07-20T00:30:36+00:00', 'source': '../data/pdf.pdf', 'creationdate': '2023-07-20T00:30:36+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'producer': 'pdfTeX-1.40.25', 'title': '', 'trapped': '/False', 'total_pages': 77, 'author': '', 'page': 8}, page_content='different down- stream model performance, highlighting the importance of data checks even when using vendors to source annotations.'), 0.015625), (Document(metadata={'page': 8, 'author': '', 'page_label': '9', 'total_pages': 77, 'subject': '', 'creationdate': '2023-07-20T00:30:36+00:00', 'creator': 'LaTeX with hyperref', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'keywords': '', 'source': '../data/pdf.pdf', 'trapped': '/False', 'title': '', 'producer': 'pdfTeX-1.40.25', 'moddate': '2023-07-20T00:30:36+00:00'}, page_content='different down- stream model performance, highlighting the importance of data checks even when using vendors to source annotations.'), 0.015625), (Document(metadata={'page_label': '2', 'creationdate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': '', 'author': '', 'subject': '', 'moddate': '2023-07-20T00:30:36+00:00', 'producer': 'pdfTeX-1.40.25', 'trapped': '/False', 'total_pages': 77, 'creator': 'LaTeX with hyperref', 'page': 1, 'keywords': '', 'source': '../data/pdf.pdf'}, page_content='. . . . . . . 72 a.6 dataset contamination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 a.7 model card . . . . . . . . . . . . . . . . . . . . . . . . . . . .'), 0.015625), (Document(metadata={'page_label': '75', 'creationdate': '2023-07-20T00:30:36+00:00', 'producer': 'pdfTeX-1.40.25', 'trapped': '/False', 'title': '', 'author': '', 'creator': 'LaTeX with hyperref', 'subject': '', 'moddate': '2023-07-20T00:30:36+00:00', 'keywords': '', 'source': '../data/pdf.pdf', 'total_pages': 77, 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'page': 74}, page_content='the training data. this was a deliberately conservative approach in order to produce a clean subset of the data with high precision, and is used in open-sourced evaluation libraries (e.g. gao et al.'), 0.015384615384615385), (Document(metadata={'creationdate': '2023-07-20T00:30:36+00:00', 'page_label': '36', 'source': '../data/pdf.pdf', 'title': '', 'total_pages': 77, 'producer': 'pdfTeX-1.40.25', 'page': 35, 'moddate': '2023-07-20T00:30:36+00:00', 'subject': '', 'trapped': '/False', 'keywords': '', 'author': '', 'creator': 'LaTeX with hyperref', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5'}, page_content='underscore various hazards likebias,toxicity,privatedataleakage,andthepotentialformalicioususes. solaimanetal.(2023)categorizes these impacts into two groups those that can be assessed within the'), 0.015384615384615385), (Document(metadata={'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creationdate': '2023-07-20T00:30:36+00:00', 'moddate': '2023-07-20T00:30:36+00:00', 'page_label': '9', 'trapped': '/False', 'total_pages': 77, 'keywords': '', 'title': '', 'source': '../data/pdf.pdf', 'page': 8, 'subject': '', 'creator': 'LaTeX with hyperref', 'author': '', 'producer': 'pdfTeX-1.40.25'}, page_content='to source annotations. tovalidateourdataquality,wecarefullyexaminedasetof180examples,comparingtheannota- tions provided by humans with the samples generated by the model through manual scrutiny.'), 0.015384615384615385), (Document(metadata={'source': '../data/pdf.pdf', 'title': '', 'trapped': '/False', 'producer': 'pdfTeX-1.40.25', 'subject': '', 'page_label': '2', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'author': '', 'creator': 'LaTeX with hyperref', 'page': 1, 'keywords': '', 'total_pages': 77, 'creationdate': '2023-07-20T00:30:36+00:00'}, page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 a.5 data annotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72 a.6 dataset'), 0.015151515151515152), (Document(metadata={'creationdate': '2023-07-20T00:30:36+00:00', 'producer': 'pdfTeX-1.40.25', 'moddate': '2023-07-20T00:30:36+00:00', 'creator': 'LaTeX with hyperref', 'page': 25, 'title': '', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '../data/pdf.pdf', 'total_pages': 77, 'subject': '', 'author': '', 'page_label': '26', 'trapped': '/False', 'keywords': ''}, page_content='appendix a.4.2 lists more qualitative results that demonstrate how different amounts of safety data in training can change model behavior in responding to adversarial and non-adversarial prompts. 0'), 0.015151515151515152), (Document(metadata={'creationdate': '2023-07-20T00:30:36+00:00', 'page_label': '69', 'source': '../data/pdf.pdf', 'total_pages': 77, 'page': 68, 'keywords': '', 'creator': 'LaTeX with hyperref', 'subject': '', 'author': '', 'moddate': '2023-07-20T00:30:36+00:00', 'title': '', 'trapped': '/False', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5'}, page_content='a.4.8 automatic safety benchmark evaluation results fine-grained analysis of toxicity, truthfulness, and bias.here we perform in-depth analyses to better understand the safety of model generations'), 0.015151515151515152), (Document(metadata={'total_pages': 77, 'subject': '', 'page': 3, 'creationdate': '2023-07-20T00:30:36+00:00', 'title': '', 'source': '../data/pdf.pdf', 'trapped': '/False', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'producer': 'pdfTeX-1.40.25', 'moddate': '2023-07-20T00:30:36+00:00', 'page_label': '4', 'creator': 'LaTeX with hyperref', 'author': '', 'keywords': ''}, page_content='methodology (section 2), fine-tuning methodology (section 3), approach to model safety (section 4), key observations and insights (section 5), relevant related work (section 6), and conclusions'), 0.014925373134328358), (Document(metadata={'page_label': '23', 'creationdate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'trapped': '/False', 'moddate': '2023-07-20T00:30:36+00:00', 'creator': 'LaTeX with hyperref', 'page': 22, 'subject': '', 'total_pages': 77, 'source': '../data/pdf.pdf', 'title': '', 'keywords': '', 'author': '', 'producer': 'pdfTeX-1.40.25'}, page_content='and the techniques we use to mitigate safety risks. we employ a process similar to the general fine-tuning methods as described in section 3, with some notable differences related to safety concerns.'), 0.014925373134328358), (Document(metadata={'creator': 'LaTeX with hyperref', 'page_label': '74', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'page': 73, 'moddate': '2023-07-20T00:30:36+00:00', 'producer': 'pdfTeX-1.40.25', 'total_pages': 77, 'title': '', 'keywords': '', 'creationdate': '2023-07-20T00:30:36+00:00', 'subject': '', 'author': '', 'source': '../data/pdf.pdf', 'trapped': '/False'}, page_content='who could work on our different data collection tasks, we conducted a multi-step assessment process where we tested their understanding of our guidelines, the alignment with our quality assessment'), 0.014925373134328358)]\\n\\nQuestion: How to prevent data contamination?\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[36;1m\u001B[1;3m[llm/end]\u001B[0m \u001B[1m[chain:RunnableSequence > llm:ChatOpenAI] [1.59s] Exiting LLM run with output:\n",
      "\u001B[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"To prevent data contamination, it is important to carefully examine a set of examples, compare annotations provided by humans with samples generated by models through manual scrutiny, and conduct data checks even when using vendors to source annotations. Additionally, employing a process similar to general fine-tuning methods with notable differences related to safety concerns can help mitigate data contamination.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"To prevent data contamination, it is important to carefully examine a set of examples, compare annotations provided by humans with samples generated by models through manual scrutiny, and conduct data checks even when using vendors to source annotations. Additionally, employing a process similar to general fine-tuning methods with notable differences related to safety concerns can help mitigate data contamination.\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 67,\n",
      "                \"prompt_tokens\": 5490,\n",
      "                \"total_tokens\": 5557,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"accepted_prediction_tokens\": 0,\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"reasoning_tokens\": 0,\n",
      "                  \"rejected_prediction_tokens\": 0\n",
      "                },\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"audio_tokens\": 0,\n",
      "                  \"cached_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"id\": \"chatcmpl-BiiyL1eMYNC1K8kFl3AfR5U45QiZG\",\n",
      "              \"service_tier\": \"default\",\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run--e2db3501-976d-4261-930f-45b9ad02c856-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 5490,\n",
      "              \"output_tokens\": 67,\n",
      "              \"total_tokens\": 5557,\n",
      "              \"input_token_details\": {\n",
      "                \"audio\": 0,\n",
      "                \"cache_read\": 0\n",
      "              },\n",
      "              \"output_token_details\": {\n",
      "                \"audio\": 0,\n",
      "                \"reasoning\": 0\n",
      "              }\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 67,\n",
      "      \"prompt_tokens\": 5490,\n",
      "      \"total_tokens\": 5557,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"accepted_prediction_tokens\": 0,\n",
      "        \"audio_tokens\": 0,\n",
      "        \"reasoning_tokens\": 0,\n",
      "        \"rejected_prediction_tokens\": 0\n",
      "      },\n",
      "      \"prompt_tokens_details\": {\n",
      "        \"audio_tokens\": 0,\n",
      "        \"cached_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0125\",\n",
      "    \"system_fingerprint\": null,\n",
      "    \"id\": \"chatcmpl-BiiyL1eMYNC1K8kFl3AfR5U45QiZG\",\n",
      "    \"service_tier\": \"default\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001B[0m[inputs]\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": \"To prevent data contamination, it is important to carefully examine a set of examples, compare annotations provided by humans with samples generated by models through manual scrutiny, and conduct data checks even when using vendors to source annotations. Additionally, employing a process similar to general fine-tuning methods with notable differences related to safety concerns can help mitigate data contamination.\"\n",
      "}\n",
      "\u001B[36;1m\u001B[1;3m[chain/end]\u001B[0m \u001B[1m[chain:RunnableSequence] [2.81s] Exiting Chain run with output:\n",
      "\u001B[0m{\n",
      "  \"output\": \"To prevent data contamination, it is important to carefully examine a set of examples, compare annotations provided by humans with samples generated by models through manual scrutiny, and conduct data checks even when using vendors to source annotations. Additionally, employing a process similar to general fine-tuning methods with notable differences related to safety concerns can help mitigate data contamination.\"\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T14:50:09.873327Z",
     "start_time": "2025-06-15T14:50:09.861091Z"
    }
   },
   "cell_type": "code",
   "source": "response1",
   "id": "a76bb18269f9d194",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To prevent data contamination, it is important to carefully examine a set of examples, compare annotations provided by humans with samples generated by models through manual scrutiny, and conduct data checks even when using vendors to source annotations. Additionally, employing a process similar to general fine-tuning methods with notable differences related to safety concerns can help mitigate data contamination.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "589afea8caeb59e5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
